{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local RAG: Build a Document Q&A System (No Cloud Required)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ThamuMnyulwa/mkdocs_rag/blob/main/notebooks/01_local_rag_no_cloud.ipynb)\n",
        "\n",
        "mkdocs_rag\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This notebook teaches RAG fundamentals using **only local tools** - no cloud account required!\n",
        "\n",
        "**Time**: 15-20 minutes | **Cost**: $0 | **Prerequisites**: None\n",
        "\n",
        "### The RAG Pipeline\n",
        "1. INGEST: Load and chunk documents\n",
        "2. EMBED: Convert text to vectors  \n",
        "3. STORE: Save in vector database\n",
        "4. RETRIEVE: Find relevant chunks\n",
        "5. GENERATE: Answer with context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install sentence-transformers faiss-cpu PyPDF2 python-docx transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "print(\"‚úÖ Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "print(f\"‚úÖ Uploaded {len(uploaded)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Extract Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_pdf(file_bytes):\n",
        "    pdf = PyPDF2.PdfReader(io.BytesIO(file_bytes))\n",
        "    return \"\\n\".join([p.extract_text() for p in pdf.pages])\n",
        "\n",
        "def read_text(file_bytes):\n",
        "    return file_bytes.decode('utf-8')\n",
        "\n",
        "documents = []\n",
        "for filename, content in uploaded.items():\n",
        "    text = read_pdf(content) if filename.endswith('.pdf') else read_text(content)\n",
        "    documents.append({'file': filename, 'text': text})\n",
        "    print(f\"üìÑ {filename}: {len(text)} chars\")\n",
        "\n",
        "print(f\"\\n‚úÖ Processed {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Chunk Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text(text, size=512, overlap=100):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), size - overlap):\n",
        "        chunk = text[i:i+size]\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "metadata = []\n",
        "for doc in documents:\n",
        "    chunks = chunk_text(doc['text'])\n",
        "    all_chunks.extend(chunks)\n",
        "    metadata.extend([doc['file']] * len(chunks))\n",
        "    print(f\"üìù {doc['file']}: {len(chunks)} chunks\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total: {len(all_chunks)} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "\n",
        "embeddings = model.encode(all_chunks, show_progress_bar=True)\n",
        "print(f\"‚úÖ Generated {len(embeddings)} embeddings ({embeddings.shape[1]} dims)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create FAISS Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings.astype('float32'))\n",
        "print(f\"‚úÖ FAISS index: {index.ntotal} vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Retrieval Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve(query, top_k=3):\n",
        "    q_emb = model.encode([query])\n",
        "    distances, indices = index.search(q_emb.astype('float32'), top_k)\n",
        "    return [(all_chunks[i], metadata[i], d) for i, d in zip(indices[0], distances[0])]\n",
        "\n",
        "# Test\n",
        "results = retrieve(\"What is this about?\")\n",
        "for i, (chunk, source, dist) in enumerate(results, 1):\n",
        "    print(f\"{i}. {source} (dist: {dist:.2f})\\n   {chunk[:100]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Load LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = pipeline('text2text-generation', model='google/flan-t5-small', max_length=512)\n",
        "print(\"‚úÖ LLM loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: RAG Q&A Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(question, top_k=3):\n",
        "    results = retrieve(question, top_k)\n",
        "    context = \"\\n\\n\".join([c for c, _, _ in results])\n",
        "    \n",
        "    prompt = f\"\"\"Answer based ONLY on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    response = generator(prompt, max_length=200)[0]['generated_text']\n",
        "    answer = response.strip()\n",
        "    sources = list(set([s for _, s, _ in results]))\n",
        "    \n",
        "    return {'answer': answer, 'sources': sources}\n",
        "\n",
        "print(\"‚úÖ RAG system ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Try It!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What is the main topic?\"\n",
        "result = ask(question)\n",
        "print(f\"‚ùì {question}\\n\")\n",
        "print(f\"üí° {result['answer']}\\n\")\n",
        "print(f\"üìö Sources: {', '.join(result['sources'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations!\n",
        "\n",
        "You built a complete RAG system from scratch!\n",
        "\n",
        "**Next Steps:**\n",
        "- Notebook 2: Vertex AI RAG Engine (managed)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
